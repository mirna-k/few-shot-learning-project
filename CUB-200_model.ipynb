{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUL0SG1cZcVH",
        "outputId": "f6130be5-8d4b-4d69-ac1f-83230a656cec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from easyFSL_helper import *\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQnrvSDxndEq"
      },
      "outputs": [],
      "source": [
        "# https://www.kaggle.com/code/wenewone/transfer-learning-example-on-cub-200-2011-dataset\n",
        "class CUB():\n",
        "    def __init__(self, root, dataset_type='train', train_ratio=1, valid_seed=123, transform=None, target_transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        df_img = pd.read_csv(os.path.join(root, 'images.txt'), sep=' ', header=None, names=['ID', 'Image'], index_col=0)\n",
        "        df_label = pd.read_csv(os.path.join(root, 'image_class_labels.txt'), sep=' ', header=None, names=['ID', 'Label'], index_col=0)\n",
        "        df_split = pd.read_csv(os.path.join(root, 'train_test_split.txt'), sep=' ', header=None, names=['ID', 'Train'], index_col=0)\n",
        "        df = pd.concat([df_img, df_label, df_split], axis=1)\n",
        "\n",
        "        df['Label'] = df['Label'] - 1\n",
        "\n",
        "        # split data\n",
        "        if dataset_type == 'test':\n",
        "            df = df[df['Train'] == 0]\n",
        "        elif dataset_type == 'train' or dataset_type == 'valid':\n",
        "            df = df[df['Train'] == 1]\n",
        "            # random split train, valid\n",
        "            if train_ratio != 1:\n",
        "                np.random.seed(valid_seed)\n",
        "                indices = list(range(len(df)))\n",
        "                np.random.shuffle(indices)\n",
        "                split_idx = int(len(indices) * train_ratio) + 1\n",
        "            elif dataset_type == 'valid':\n",
        "                raise ValueError('train_ratio should be less than 1!')\n",
        "            if dataset_type == 'train':\n",
        "                df = df.iloc[indices[:split_idx]]\n",
        "            else:       # dataset_type == 'valid'\n",
        "                df = df.iloc[indices[split_idx:]]\n",
        "        else:\n",
        "            raise ValueError('Unsupported dataset_type!')\n",
        "        self.img_name_list = df['Image'].tolist()\n",
        "        self.label_list = df['Label'].tolist()\n",
        "        # Convert greyscale images to RGB mode\n",
        "        self._convert2rgb()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_list)\n",
        "    \n",
        "    def get_labels(self):\n",
        "        return self.label_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, 'images', self.img_name_list[idx])\n",
        "        image = Image.open(img_path)\n",
        "        target = self.label_list[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            target = self.target_transform(target)\n",
        "        return image, target\n",
        "\n",
        "    def _convert2rgb(self):\n",
        "        for i, img_name in enumerate(self.img_name_list):\n",
        "            img_path = os.path.join(self.root, 'images', img_name)\n",
        "            image = Image.open(img_path)\n",
        "            color_mode = image.mode\n",
        "            if color_mode != 'RGB':\n",
        "                self.img_name_list[i] = img_name.replace('.jpg', '_rgb.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((180, 180)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((180, 180)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "z_UCO1zqZk65",
        "outputId": "7573fc85-0be1-44e8-956b-36056dfb948e"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "path = 'archive\\CUB_200_2011'\n",
        "\n",
        "NUM_WORKERS = 4\n",
        "SPLIT_RATIO = 0.9\n",
        "RANDOM_SEED = 123\n",
        "CLASS_NUM = 200\n",
        "\n",
        "N = 5\n",
        "K = 5\n",
        "n_query = 10\n",
        "n_tasks_per_epoch = 100\n",
        "n_validation_tasks = 50\n",
        "\n",
        "# create dataset\n",
        "train_set = CUB(path, 'train', SPLIT_RATIO, RANDOM_SEED, transform=train_transform)\n",
        "val_set = CUB(path, 'valid', SPLIT_RATIO, RANDOM_SEED, transform=val_transform)\n",
        "\n",
        "print(\"Train: {}\".format(len(train_set)))\n",
        "print(\"Valid: {}\".format(len(val_set)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_sampler = TaskSampler(train_set, n_way=N, n_shot=K, n_query=n_query, n_tasks=n_tasks_per_epoch)\n",
        "val_sampler = TaskSampler(val_set, n_way=N, n_shot=K, n_query=n_query, n_tasks=n_validation_tasks)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_sampler=train_sampler,\n",
        "    pin_memory=True,\n",
        "    collate_fn=train_sampler.episodic_collate_fn,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_set,\n",
        "    batch_sampler=val_sampler,\n",
        "    pin_memory=True,\n",
        "    collate_fn=val_sampler.episodic_collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrototypicalNetworks(nn.Module):\n",
        "    def __init__(self, backbone: nn.Module):\n",
        "        super(PrototypicalNetworks, self).__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        support_images: torch.Tensor,\n",
        "        support_labels: torch.Tensor,\n",
        "        query_images: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        # Extract the features of support and query images\n",
        "        z_support = self.backbone.forward(support_images)\n",
        "        z_query = self.backbone.forward(query_images)\n",
        "\n",
        "        # Infer the number of different classes from the labels of the support set\n",
        "        n_way = len(torch.unique(support_labels))\n",
        "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
        "        z_proto = torch.cat(\n",
        "            [\n",
        "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
        "                for label in range(n_way)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Compute the euclidean distance from queries to prototypes\n",
        "        dists = torch.cdist(z_query, z_proto)\n",
        "\n",
        "        scores = -dists\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet = models.resnet34()\n",
        "few_shot_classifier = PrototypicalNetworks(resnet).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_module = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(few_shot_classifier.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, scheduler, train_data_loader, loss_module, num_epochs=100, logging_dir='runs/our_experiment'):\n",
        "    os.makedirs(logging_dir, exist_ok=True)\n",
        "    writer = SummaryWriter(logging_dir)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for support_images, support_labels, query_images, query_labels, _ in tqdm(train_data_loader, 'Epoch %d'%(epoch + 1)):\n",
        "            ## Step 2: Run the model on the input data\n",
        "            classification_scores = model(support_images.to(device), support_labels.to(device), query_images.to(device))\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(classification_scores, query_labels.to(device))\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item() * len(support_images)\n",
        "\n",
        "        # Add average loss to TensorBoard\n",
        "        epoch_loss /= len(train_data_loader.dataset)\n",
        "\n",
        "        # Calling scheduler\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "        validation_accuracy = evaluate(few_shot_classifier, val_loader, device=device, tqdm_prefix=\"Validation\")\n",
        "\n",
        "        if validation_accuracy > best_validation_accuracy:\n",
        "            best_validation_accuracy = validation_accuracy\n",
        "            best_state = copy.deepcopy(few_shot_classifier.state_dict())\n",
        "            # state_dict() returns a reference to the still evolving model's state so we deepcopy\n",
        "            # https://pytorch.org/tutorials/beginner/saving_loading_models\n",
        "            print(\"Found a new best model.\")\n",
        "\n",
        "        writer.add_scalar('training_loss',\n",
        "                          epoch_loss,\n",
        "                          global_step = epoch + 1)\n",
        "        writer.add_scalar(\"Val_acc\", validation_accuracy, epoch)\n",
        "\n",
        "        model_dir = logging_dir.split('/')[-1]\n",
        "        os.makedirs(f'models/{model_dir}', exist_ok=True)\n",
        "        if((epoch + 1) % 10 == 0):\n",
        "          torch.save(model.state_dict(), f'models/{model_dir}/model{epoch + 1}.pt')\n",
        "\n",
        "    writer.close()\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_model(few_shot_classifier, optimizer, scheduler, train_loader, loss_module, num_epochs=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
